{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# create workspace\r\n",
        "# azureml-core of version 1.0.72 or higher is required\r\n",
        "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\r\n",
        "from azureml.core import Workspace, Dataset, Datastore, ScriptRunConfig\r\n",
        "from azureml.data.data_reference import DataReference\r\n",
        "import os\r\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1606763201986
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pipeline\r\n",
        "from azureml.core.compute import AmlCompute, ComputeTarget\r\n",
        "from azureml.pipeline.steps import PythonScriptStep\r\n",
        "from azureml.pipeline.core import Pipeline\r\n",
        "from azureml.data.datapath import DataPath, DataPathComputeBinding\r\n",
        "from azureml.pipeline.core import PipelineParameter\r\n",
        "from azureml.core.datastore import Datastore\r\n",
        "from azureml.pipeline.core import PipelineData\r\n",
        "from azureml.data import OutputFileDatasetConfig\r\n",
        "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
        "from azureml.core import Environment\r\n",
        "\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "datastore = ws.get_default_datastore()\r\n",
        "\r\n",
        "#Retrieve an already attached Azure Machine Learning Compute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "aml_compute_target = \"Your AML Compute\"\r\n",
        "try:\r\n",
        "    aml_compute = AmlCompute(ws, aml_compute_target)\r\n",
        "    print(\"Found existing compute target: {}\".format(aml_compute_target))\r\n",
        "except ComputeTargetException:\r\n",
        "    print(\"Creating new compute target: {}\".format(aml_compute_target))\r\n",
        "    \r\n",
        "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\r\n",
        "                                                                min_nodes = 1, \r\n",
        "                                                                max_nodes = 1)    \r\n",
        "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\r\n",
        "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\r\n",
        "\r\n",
        "# get datasets from CI\r\n",
        "# Ad hoc Datasets also referred to Saved datasets are not discoverable from Audience Insights currently.\r\n",
        "# Example: IncompatibleDataset = Dataset.Tabular.from_delimited_files('https://cidatasets.blob.core.windows.net/hotel-scenario/ContosoHotel_HotelActivity_1.csv')\r\n",
        "# You need to use registered datasets and then consume them by their name \r\n",
        "# Example: CompatibleDataset = Dataset.get_by_name(ws, name='Hotel Stay Activity Data')\r\n",
        "\r\n",
        "#HotelStayActivity_dataset = Dataset.Tabular.from_delimited_files('https://cidatasets.blob.core.windows.net/hotel-scenario/ContosoHotel_HotelActivity_1.csv')\r\n",
        "HotelStayActivity_dataset = Dataset.get_by_name(ws, name='Hotel Stay Activity Data')\r\n",
        "HotelStayActivity_pipeline_param = PipelineParameter(name=\"HotelStayActivity_pipeline_param\", default_value=HotelStayActivity_dataset)\r\n",
        "HotelStayActivity_ds_consumption = DatasetConsumptionConfig(\"HotelStayActivity_dataset\", HotelStayActivity_pipeline_param)\r\n",
        "\r\n",
        "#Customer_dataset = Dataset.Tabular.from_delimited_files('https://cidatasets.blob.core.windows.net/hotel-scenario/Customer_1.csv')    \r\n",
        "Customer_dataset = Dataset.get_by_name(ws, name= 'Customer Data')\r\n",
        "Customer_pipeline_param = PipelineParameter(name=\"Custolmer_pipeline_param\", default_value=Customer_dataset)\r\n",
        "Customer_ds_consumption = DatasetConsumptionConfig(\"Customer_dataset\", Customer_pipeline_param)\r\n",
        "\r\n",
        "#ServiceUsage_dataset = Dataset.Tabular.from_delimited_files('https://cidatasets.blob.core.windows.net/hotel-scenario/ContosoHotel_ServiceUsage_1.csv')    \r\n",
        "ServiceUsage_dataset = Dataset.get_by_name(ws, name='Service Usage Data')\r\n",
        "ServiceUsage_pipeline_param = PipelineParameter(name=\"ServiceUsage_pipeline_param\", default_value=ServiceUsage_dataset)\r\n",
        "ServiceUsage_ds_consumption = DatasetConsumptionConfig(\"ServiceUsage_dataset\", ServiceUsage_pipeline_param)\r\n",
        "\r\n",
        "# Currently OutputFIleDatasetConfig doesn't support parameterization of the output datastore and path which are requried for Audience Insights Integration\r\n",
        "# Example: hotel_churn_ds = OutputFileDatasetConfig(name=\"HotelChurnOutput\", destination=(datastore,\"path\")).read_delimited_files().register_on_complete(name='HotelChurnOutput')\r\n",
        "# Instead, you you will need to use string parameters for datastore and path, and then rely on Datastore.upload() to upload it to the datastore and path, which is accessible to Audience Insights.\r\n",
        "# In your Pipeline Setup: OutputPathParameter = PipelineParameter(name=\"output_path\", default_value=\"HotelChurnOutput1/HotelChurnOutput.csv\")\r\n",
        "# In your score.py: ws = run.experiment.workspace\r\n",
        "#                   datastore = Datastore.get(ws, datastore_parameter)\r\n",
        "#                   datastore.upload(src_dir=compute_src_dir, target_path=output_path, overWrite=True)\r\n",
        "\r\n",
        "OutputPathParameter = PipelineParameter(name=\"output_path\", default_value=\"HotelChurnOutput2/HotelChurnOutput.csv\")\r\n",
        "OutputDatastoreParameter = PipelineParameter(name=\"output_datastore\", default_value=\"workspaceblobstore\")\r\n",
        "\r\n",
        "env = Environment.from_conda_specification(name = 'env', file_path = './HotelChurnInferencePipeline.yml')\r\n",
        "\r\n",
        "# source_directory\r\n",
        "source_directory = './scripts'\r\n",
        "os.makedirs(source_directory, exist_ok=True)\r\n",
        "script_name = \"score.py\"\r\n",
        "# define a single step pipeline for demonstration purpose.\r\n",
        "src = ScriptRunConfig(source_directory=source_directory,\r\n",
        "                    script=script_name,\r\n",
        "                    compute_target=aml_compute_target,\r\n",
        "                    environment=env)\r\n",
        "\r\n",
        "inferenceStep = PythonScriptStep(\r\n",
        "    name=\"Inferencing_Step\",\r\n",
        "    script_name=src.script,\r\n",
        "    arguments=[\"--input_data1\", HotelStayActivity_ds_consumption, \"--input_data2\", Customer_ds_consumption, \"--input_data3\", ServiceUsage_ds_consumption, \"--output_path\", OutputPathParameter, \"--output_datastore\", OutputDatastoreParameter],\r\n",
        "    inputs=[HotelStayActivity_ds_consumption, Customer_ds_consumption, ServiceUsage_ds_consumption], \r\n",
        "    #compute_target=aml_compute_target, \r\n",
        "    source_directory=src.source_directory,\r\n",
        "    runconfig = src.run_config)\r\n",
        "\r\n",
        "print(\"Inferencing_Step created\")\r\n",
        "# build and validate Pipeline\r\n",
        "pipeline = Pipeline(workspace=ws, steps=[inferenceStep])\r\n",
        "print(\"Pipeline is built\")"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606773079458
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#register model\r\n",
        "\r\n",
        "from azureml.core.model import Model\r\n",
        "# Tip: When model_path is set to a directory, you can use the child_paths parameter to include\r\n",
        "#      only some of the files from the directory\r\n",
        "model = Model.register(model_path = \"./models\",\r\n",
        "                       model_name = \"churnscore\",\r\n",
        "                       description = \"Logistic Regression Model\",\r\n",
        "                       workspace = ws)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606437992002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# publish pipeline endpoint\r\n",
        "from azureml.pipeline.core import PipelineEndpoint\r\n",
        "#pipeline_endpoint = PipelineEndpoint.publish(workspace=ws, name=\"HotelChurnPipelineEndpoint\", pipeline=pipeline, description=\"Hotel Churn Pipeline Endpoint\")\r\n",
        "published_pipeline = pipeline.publish(name=\"HotelChurnPipeline\", description=\"published pipeline\")\r\n",
        "pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=\"HotelChurnPipelineEndpoint\") \r\n",
        "pipeline_endpoint.add_default(pipeline=published_pipeline)"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606773092349
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\r\n",
        "pipeline_endpoint = PipelineEndpoint.get(workspace=ws, name=\"HotelChurnPipelineEndpoint\")\r\n",
        "experiment = Experiment(ws, 'my-experiments')\r\n",
        "pipeline_run = experiment.submit(config=pipeline_endpoint)\r\n",
        "pipeline_run.get_details()"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1606773094817
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
